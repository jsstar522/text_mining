{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crawling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsstar522/text_mining/blob/master/01_Basic/Crawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4ndO2EeP5ECL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Crawling for Python"
      ]
    },
    {
      "metadata": {
        "id": "pbjBQz6kRFXZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Package 설치"
      ]
    },
    {
      "metadata": {
        "id": "YCpb0E4PQ9uY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Beatiful Soup 4\n",
        "로컬로 HTML문서를 가져올 수 있는 HTML parser입니다."
      ]
    },
    {
      "metadata": {
        "id": "cFtRLUhqQxal",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### LXML\n",
        "로컬로 XML문서를 가져올 수 있는 XML parser입니다."
      ]
    },
    {
      "metadata": {
        "id": "6QJLcphjXyEK",
        "colab_type": "code",
        "outputId": "513f5679-6e6d-4da4-a1b1-eb0bca1be634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install lxml"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lxml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/a4/9eea8035fc7c7670e5eab97f34ff2ef0ddd78a491bf96df5accedb0e63f5/lxml-4.2.5-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.8MB 4.9MB/s \n",
            "\u001b[?25hInstalling collected packages: lxml\n",
            "Successfully installed lxml-4.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gQPhnEFyRwPY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Requests\n",
        "\n",
        "HTTP를 이용해서 웹과 통신하는 Package입니다."
      ]
    },
    {
      "metadata": {
        "id": "HTwWdOVpRZgf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 네이버 영화 페이지에서 영화제목 가져오기\n",
        "\n",
        "네이버 영화 페이지의 **보헤미안 렙소디** : https://movie.naver.com/movie/bi/mi/basic.nhn?code=156464\n",
        "\n",
        "이렇게 다양한 list를 보여주는 대부분의 웹페이지는 페이지 url마다 숫자로 된 id를 가지고 있습니다. 하지만 그렇지 않은 웹페이지도 많기 때문에 **페이지를 잘 살펴보는 것이 크롤링의 첫번째 과제입니다**. 영화제목을 가져오기 위해서는 이렇게 영화목록을 클릭해서 제목을 가져와야합니다. 즉, 다양한 페이지의 url_id를 변수로 둬야합니다.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8lsMAKbpUvLK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "import lxml\n",
        "import sys\n",
        "from pprint import pprint\n",
        "## pprint = pretty print, 더 깔끔한 출력을 위한 module\n",
        "\n",
        "page_url_base = ' https://movie.naver.com/movie/bi/mi/basic.nhn?code=%s'\n",
        "\n",
        "movie_url_id = 156464"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "orK1MLFtVNxp",
        "colab_type": "code",
        "outputId": "0fdf453d-3415-4239-9686-e8d933d294ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "page_url = page_url_base % movie_url_id\n",
        "\n",
        "## 해당 url 웹페이지의 정보를 가져온다\n",
        "r = requests.get(page_url)\n",
        "\n",
        "html = r.text\n",
        "\n",
        "## requests로 가져온 페이지 정보를 lxml로 보기좋게 구조화\n",
        "page = BeautifulSoup(html, 'lxml')\n",
        "\n",
        "r.headers\n",
        "\n",
        "\n",
        "## lxml 사용 오류가 날 때, 런타임 재시작할 것"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Date': 'Mon, 03 Dec 2018 17:01:59 GMT', 'Pragma': 'no-cache', 'Expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'Cache-Control': 'no-cache, no-store', 'Content-Language': 'ko-KR', 'P3P': 'CP=\"ALL CURa ADMa DEVa TAIa OUR BUS IND PHY ONL UNI PUR FIN COM NAV INT DEM CNT STA POL HEA PRE LOC OTC\", CP=\"ALL CURa ADMa DEVa TAIa OUR BUS IND PHY ONL UNI PUR FIN COM NAV INT DEM CNT STA POL HEA PRE LOC OTC\"', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip', 'Content-Length': '38192', 'Content-Type': 'text/html;charset=UTF-8', 'Referrer-Policy': 'unsafe-url', 'Server': 'nfront'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "t3t7n5WhWWTB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "해당 페이지의 HTML 문서를 가져오기 위해서 BeatifulSoup, Requests, LXML을 사용했습니다. \n",
        "\n",
        "r.headers로 메타데이터를 확인할 수 있습니다.\n",
        "\n",
        "이제 영화 제목을 가져와야하니, 영화 제목이 html 문서에서 어떤 곳에 표현 되어있는지 확인하겠습니다."
      ]
    },
    {
      "metadata": {
        "id": "jn-CduKc4XNO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![movie_page.png](movie_page.png)"
      ]
    },
    {
      "metadata": {
        "id": "m8_VVotn4Y_8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "html은 수많은 태그와 링크들이 나뭇가지처럼 트리구조를 이루고 있습니다.\n",
        "\n",
        "그리고 BeautifulSoup의 select라는 함수는 수많은 가지들 중 하나의 잎을 찾을 수 있게 해줍니다.\n",
        "\n",
        "원하는 잎을 찾기 위해선 그 잎이 가지고 있는 고유의 class name, xpath 중 하나를 알아야합니다.\n",
        "\n",
        "하지만 이 모든게 명확하지 않을 때에는 **태그들이 이루는 구조**를 알고 있으면 원하는 잎을 찾을 수 있습니다.\n",
        "\n",
        "**영화 제목은 특별한 class name을 가지고 있지 않습니다. 그래서 그 상위에 있는 태그의 class로 수사망을 좁혀보겠습니다.**"
      ]
    },
    {
      "metadata": {
        "id": "yDN2zYhuYy2r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c159e466-edd6-472d-a0da-f7a36bf05ec3"
      },
      "cell_type": "code",
      "source": [
        "title = page.select('div[class=mv_info] a')\n",
        "print(type(title))\n",
        "print(len(title))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oGEOKJl55Wl3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**BeautifulSoup.select() 메서드 안에 parameter는 찾고자 하는 정보의 태그 경로를 써줍니다.** 처음부터(head 태그부터) 다 써줄 필요는 없습니다. select 메서드가 혼란을 겪지 않도록 중복되지 않는 class name 부터 경로를 지정해주면 됩니다. (참고로 select 메서드는 html문서를 처음부터 끝까지 scanning하면서 parameter로 받은 경로를 모두 뽑아냅니다.)\n",
        "\n",
        "div 태그 중 'my_info'라는 class 밑에 있는 a 태그는 총 64개가 나오는군요. 우리는 제목만 필요한데, 제목을 알려주는 경로와 겹치는 정보들이 63개나 더 있습니다.\n",
        "(아무래도 제목은 특별한 class를 쓰지 않고 그냥 text로 적었다는 것을 예측할 수 있습니다.)\n",
        "\n",
        "범위를 좁혀볼까요? 잘보면 'my_info'라는 class 밑에 있는 'h_movie'라는 클래스. 그 밑에 있는 a 태그는 두개 밖에 안보이는군요.\n",
        "\n",
        "그렇다면 어떻게 제목부분만 뽑아낼까. html 문서를 잘 들여다 봅시다.\n",
        "제목을 적은 a태그는 'h_movie'라는 class 바로 아래에 있습니다.\n",
        "\n",
        "즉, title에 저장된 2개의 list 중 첫번째 list에 제목이 들어가 있습니다\n"
      ]
    },
    {
      "metadata": {
        "id": "XDPGsg_CAnng",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04680c65-0853-43d3-a593-3d39c957681d"
      },
      "cell_type": "code",
      "source": [
        "title = page.select('div[class=mv_info] h3[class=h_movie] a')\n",
        "print(len(title))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uLb-9M1sBxYq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "출력해보니 같은 경로를 가진 정보가 4개가 있다고 하네요. 그래서 찾아보니 중복된 정보가 하나씩 더 들어가 있네요.\n",
        "\n",
        "이유는 html문서를 더 파내야 알 수 있겠지만 느낌상 포스터를 클릭했을 때 뜨는 팝업창이나 예매를 누르면 뜨는 팝업창에 같은 정보를 넣기 위함인 것 같습니다.\n",
        "\n",
        "크롤링은 이렇게 시행착오가 많이 필요합니다."
      ]
    },
    {
      "metadata": {
        "id": "qD3grweq3zCf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4dd115b6-848c-401f-fbed-cd14822529aa"
      },
      "cell_type": "code",
      "source": [
        "print(title[0],'\\n')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<a href=\"./basic.nhn?code=156464\">보헤미안 랩소디</a> \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BCOktjiW_k2c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "원하는 제목부분을 html에서 그대로 떠왔으니 이제 제목부분\"만\" 남겨야겠죠.\n",
        "\n",
        "**BeautifulSoup의 메서드 중에서 text를 이용해 태그 안에 있는 text만 남겨보겠습니다.**"
      ]
    },
    {
      "metadata": {
        "id": "-mdWldJp2zAR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a2ee592-436b-45a2-f36d-3973c9b5acd5"
      },
      "cell_type": "code",
      "source": [
        "print(title[0].text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "보헤미안 랩소디\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uPdB8b8uCnno",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "크롤러은 이렇게 html 구조만 알면 만들기 어렵지 않습니다. 그리고 약간의 인내심이 필요합니다.\n",
        "\n",
        "잘 만들어져있는 사이트일수록 템플릿을 사용하기 때문에 영화의 모든 제목은 저 태그경로로 모두 뽑아낼 수 있습니다."
      ]
    },
    {
      "metadata": {
        "id": "e8h5NCG8DM5O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 영화제목 여러개 뽑아오기"
      ]
    },
    {
      "metadata": {
        "id": "suRLzZQ4DWDk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "하지만 꼭 템플릿이 모두 같다는 보장은 없습니다. 그래서 우리는 크롤링을 할 때 try - except 구문을 자주 사용합니다.\n",
        "찾고자 하는 태그가 없으면 작업이 끊기지 않고 넘어가죠.\n",
        "\n",
        "이제 영화제목을 뽑아낼 수 있는 함수를 만들어 여러 페이지에서 실행 시켜보겠습니다.\n",
        "**원하는 결과가 많을 때는 함수를 여러개로 만들어 정리해 놓는 습관을 들이는게 좋습니다.**\n",
        "\n",
        "page id를 증가시키면서 다양한 페이지를 볼텐데요. 하지만 page id가 순서대로 모두 존재하지는 않습니다. 그래서 우리는 try - except 구문을 사용할 것이고 없는 페이지는 그냥 넘겨버리죠."
      ]
    },
    {
      "metadata": {
        "id": "r5rhgYqJHAmV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "page_url_base = ' https://movie.naver.com/movie/bi/mi/basic.nhn?code=%s'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A0AumGTuEW6i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_page(page_url):\n",
        "    try:\n",
        "        r = requests.get(page_url)\n",
        "        return BeautifulSoup(r.text, 'lxml')\n",
        "    \n",
        "    ## 페이지가 없을 때\n",
        "    except Exception as e:\n",
        "        pprint('오류', e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bIb6Dm5oFzEH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_title(page):\n",
        "    try:\n",
        "        return page.select('div[class=mv_info] h3[class=h_movie] a')[0].text\n",
        "    except Exception as e:\n",
        "        pprint('오류', e)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZPoETAjnGNnk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d8766229-f92d-47ce-f967-49a2ae79e9be"
      },
      "cell_type": "code",
      "source": [
        "for movie_id in range(156454, 156465):\n",
        "    page_url = page_url_base % movie_id\n",
        "    page = get_page(page_url)\n",
        "    title = get_title(page)\n",
        "    pprint(title)\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'시각탐정 히구라시 타비토'\n",
            "'살인편차치70'\n",
            "'나이프의 행방'\n",
            "'60분 드라마 2016'\n",
            "'막부 말기 미식가 무사의 밥'\n",
            "'시간의 습속'\n",
            "'리테이크'\n",
            "'강철의 연금술사'\n",
            "'나에게 운명의 사랑이란 있을 수 없다고 생각했다'\n",
            "'무한의 주인'\n",
            "'보헤미안 랩소디'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wsL2ofLTHBwe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "이렇게 순식간에, 그리고 보기 편하게 영화들의 제목을 뽑아낼 수 있습니다. 하지만 이렇게 크롤러를 돌리다보면 서버의 트래픽이  굉장히 높아져 서버관리자가 의도적으로 IP를 차단할 수 있습니다. 그렇기 때문에 **적당한 break를 하나의 루프가 돌때마다 적용시켜 크롤링을 하는 것이 바람직합니다.**"
      ]
    },
    {
      "metadata": {
        "id": "RJCIdc8eIWMU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6e2a9364-3c61-4efb-bc91-ee9dd2e7ba1c"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "for movie_id in range(156454, 156465):\n",
        "    time.sleep(1)\n",
        "    \n",
        "    page_url = page_url_base % movie_id\n",
        "    page = get_page(page_url)\n",
        "    title = get_title(page)\n",
        "    pprint(title)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'시각탐정 히구라시 타비토'\n",
            "'살인편차치70'\n",
            "'나이프의 행방'\n",
            "'60분 드라마 2016'\n",
            "'막부 말기 미식가 무사의 밥'\n",
            "'시간의 습속'\n",
            "'리테이크'\n",
            "'강철의 연금술사'\n",
            "'나에게 운명의 사랑이란 있을 수 없다고 생각했다'\n",
            "'무한의 주인'\n",
            "'보헤미안 랩소디'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}